{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6QWV-n9BAnH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from numpy import mean\n",
        "from matplotlib import pyplot as plt\n",
        "!pip install antropy\n",
        "import antropy as an\n",
        "from scipy.stats import skew, kurtosis\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from keras import layers, models, regularizers\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.metrics import roc_curve,auc, precision_score,recall_score,f1_score"
      ],
      "metadata": {
        "id": "eZlP24TnwK5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features_df = pd.read_csv('/content/extractedFeatures.csv')\n",
        "Features_df.drop(Features_df.columns[0], axis = 1, inplace = True)      # In the csv file first column has number values from 0-2280 and carries redundadnt info.\n",
        "\n",
        "#3 Data preparation\n",
        "##3.1 Shuffling\n",
        "Data = Features_df.sample(frac = 1)\n",
        "features = Data[[x for x in Data.columns if x not in [\"Label\"]]]   # Data for training\n",
        "Labels = Data['Label']                                            # Labels for training\n",
        "Labels = Labels.astype('category')"
      ],
      "metadata": {
        "id": "oIJkgxv5IwOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitRatio = 0.3\n",
        "train, test = train_test_split(Data ,test_size=splitRatio,\n",
        "                               random_state = 123, shuffle = True)  # Spilt to training and testing data\n",
        "\n",
        "train_X = train[[x for x in train.columns if x not in [\"Label\"]]]   # Data for training\n",
        "train_Y = train['Label']                                            # Labels for training\n",
        "\n",
        "###4.5.2 Testing Data\n",
        "test_X = test[[x for x in test.columns if x not in [\"Label\"]]]     # Data fo testing\n",
        "test_Y = test[\"Label\"]                                              # Labels for training\n",
        "\n",
        "###4.5.3 Validation Data\n",
        "x_val = train_X[:200]                                                # 50 Sample for Validation\n",
        "partial_x_train = train_X[200:]\n",
        "partial_x_train = partial_x_train.values\n",
        "\n",
        "y_val = train_Y[:200]\n",
        "y_val = to_categorical(y_val)\n",
        "partial_y_train = train_Y[200:]\n",
        "partial_y_train = partial_y_train.values\n",
        "partial_y_train = to_categorical(partial_y_train)\n",
        "\n",
        "print(\"Data is prepeared\")\n",
        "\n",
        "print(\"Start Building Classifer\")\n",
        "\n",
        "#4 Classification Model\n",
        "\n",
        "##4.1 Building Model\n",
        "\n",
        "###4.1.1 Architecture\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(200, activation = 'relu', input_shape=(17,),kernel_regularizer=regularizers.l1(0.01)))\n",
        "# model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(150, activation = 'relu'))\n",
        "model.add(layers.Dense(100, activation = 'relu'))\n",
        "# model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(75, activation = 'relu'))\n",
        "model.add(layers.Dense(48,  activation= 'sigmoid'))\n",
        "\n",
        "####5.1.1.2 Hyper Parameters Tuning\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Classifier Bulit\\n\")\n",
        "print(\"Start Training\\n\")\n",
        "\n",
        "##5.1.2 Training Model\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs = 50,                # initial value = 1000\n",
        "                    batch_size = 16,              # initial value = 16\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "weights = model.get_weights()\n",
        "configs = model.get_config()\n",
        "\n",
        "# Featuers_weights = np.apply_along_axis(mean, 1, weights[0])\n",
        "\n",
        "\n",
        "\n",
        "print(\"Finish Training\")\n",
        "\n",
        "\n",
        "#5 Model Evaluation\n",
        "\n",
        "##5.1 Network Architecture\n",
        "print(model.summary())\n",
        "\n",
        "print(\"Start Evaluating Data\")\n",
        "\n",
        "##4.2 Training Process\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss_values)+1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss_values, 'bo', label=\"training loss\", color='r')\n",
        "plt.plot(epochs, val_loss_values, 'b', label=\"validation loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoches\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "plt.figure()\n",
        "plt.plot(epochs, acc_values, 'bo', label=\"training acc\", color = 'r')\n",
        "plt.plot(epochs, val_acc_values, 'b', label=\"validation acc\")\n",
        "plt.title(\"Training and Validation acc\")\n",
        "plt.xlabel(\"Epoches\")\n",
        "plt.ylabel(\"acc\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "##5.3 Prediction\n",
        "ANN_predictions = model.predict(test_X)\n",
        "\n",
        "Pred = np.zeros([len(ANN_predictions)])\n",
        "for i in range(0,len(ANN_predictions)):\n",
        "    Pred[i] = list(ANN_predictions[i]).index(max(ANN_predictions[i]))\n",
        "ANN_Pred = pd.Series(Pred)\n",
        "\n",
        "####5.1.2.4 Metrics\n",
        "print(\"Accuracy:\",accuracy_score(test_Y, ANN_Pred))\n",
        "print(\"f1 score:\", f1_score(test_Y, ANN_Pred,average=\"micro\"))\n",
        "print(\"precision score:\", precision_score(test_Y, ANN_Pred,average=\"micro\"))\n",
        "print(\"recall score:\", recall_score(test_Y, ANN_Pred,average=\"micro\"))\n",
        "print(\"confusion matrix:\\n\",confusion_matrix(test_Y, ANN_Pred))\n",
        "print(\"classification report:\\n\", classification_report(test_Y, ANN_Pred))"
      ],
      "metadata": {
        "id": "gduG_XqABFpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_X)"
      ],
      "metadata": {
        "id": "kxcC3DJKDJqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import XGBoost library\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "# Define and train an XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=48, random_state=123)\n",
        "xgb_classifier.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "xgb_predictions = xgb_classifier.predict(test_X)\n",
        "\n",
        "# Evaluate the XGBoost model\n",
        "accuracy = accuracy_score(test_Y, xgb_predictions)\n",
        "f1 = f1_score(test_Y, xgb_predictions, average=\"micro\")\n",
        "precision = precision_score(test_Y, xgb_predictions, average=\"micro\")\n",
        "recall = recall_score(test_Y, xgb_predictions, average=\"micro\")\n",
        "conf_matrix = confusion_matrix(test_Y, xgb_predictions)\n",
        "classification_rep = classification_report(test_Y, xgb_predictions)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"XGBoost Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "id": "JPWOY3JbB2hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "Data1 = pd.read_csv('/content/extractedFeatures.csv')\n",
        "X= Data1[[x for x in Data.columns if x not in [\"Label\"]]]\n",
        "Y=Data1['Label']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n",
        "\n",
        "# Define and train multiple classifiers\n",
        "classifiers = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=123),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=123),\n",
        "    'SVM': SVC(random_state=123),\n",
        "    # Add more classifiers as needed\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, classifier in classifiers.items():\n",
        "    classifier.fit(X_train, Y_train)\n",
        "    predictions = classifier.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(Y_test, predictions)\n",
        "    f1 = f1_score(Y_test, predictions, average='micro')\n",
        "    precision = precision_score(Y_test, predictions, average='micro')\n",
        "    recall = recall_score(Y_test, predictions, average='micro')\n",
        "    conf_matrix = confusion_matrix(Y_test, predictions)\n",
        "    classification_rep = classification_report(Y_test, predictions)\n",
        "\n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'Confusion Matrix': conf_matrix,\n",
        "        'Classification Report': classification_rep\n",
        "    }\n",
        "\n",
        "# Print the results for each classifier\n",
        "for name, metrics in results.items():\n",
        "    print(f'{name} Metrics:')\n",
        "    print(f'Accuracy: {metrics[\"Accuracy\"]}')\n",
        "    print(f'F1 Score: {metrics[\"F1 Score\"]}')\n",
        "    print(f'Precision: {metrics[\"Precision\"]}')\n",
        "    print(f'Recall: {metrics[\"Recall\"]}')\n",
        "    print(f'Confusion Matrix:\\n{metrics[\"Confusion Matrix\"]}')\n",
        "    print(f'Classification Report:\\n{metrics[\"Classification Report\"]}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "kK9nfT3jBLkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Notice:\n",
        "[1] This code is only for reference.\n",
        "Please modify the codes to fit your own data.\n",
        "[2] The Code is based on TensorFlow 2.X.\n",
        "Please install the TensorFlow 2.X version.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "# Read Training Data\n",
        "train_data = train_X\n",
        "train_data = np.array(train_data).astype('float32')\n",
        "# Read Training Labels\n",
        "train_labels = train_Y\n",
        "train_labels = np.array(train_labels).astype('float32')\n",
        "train_labels = tf.one_hot(indices=train_labels, depth=2)\n",
        "train_labels = np.squeeze(train_labels)\n",
        "\n",
        "# Read Testing Data\n",
        "test_data = test_X\n",
        "test_data = np.array(test_data).astype('float32')\n",
        "\n",
        "# Read Testing Labels\n",
        "test_labels = test_Y\n",
        "test_labels = np.array(test_labels).astype('float32')\n",
        "test_labels = tf.one_hot(indices=test_labels, depth=2)\n",
        "test_labels = np.squeeze(test_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CatgoricalTP(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='categorical_tp', **kwargs):\n",
        "        super(CatgoricalTP, self).__init__(name=name, **kwargs)\n",
        "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        y_true = tf.argmax(y_true, axis=-1)\n",
        "        values = tf.equal(tf.cast(y_pred, 'int32'), tf.cast(y_true, 'int32'))\n",
        "        values = tf.cast(values, 'float32')\n",
        "        if sample_weight is not None:\n",
        "            sample_weights = tf.cast(sample_weight, 'float32')\n",
        "            values = tf.multiply(values, sample_weights)\n",
        "\n",
        "        self.tp.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "    def result(self):\n",
        "        return self.tp\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.tp.assign(0.)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.5):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, x):\n",
        "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = tf.reshape(x, [-1, self.maxlen, self.embed_dim])\n",
        "        out = x + positions\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "maxlen = 17  # (Maximum) length of the signals\n",
        "embed_dim = 1  # Number of features of one time point\n",
        "num_heads = 8  # Number of attention heads\n",
        "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    # Input Time-series\n",
        "    inputs = layers.Input(shape=(maxlen * embed_dim,))\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    # Encoder Architecture\n",
        "    transformer_block_1 = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
        "    transformer_block_2 = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
        "    x = transformer_block_1(x)\n",
        "    x = transformer_block_2(x)\n",
        "\n",
        "    # Output\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\", CatgoricalTP()])\n",
        "\n",
        "history = model.fit(\n",
        "    train_data, train_labels, batch_size=64, epochs=100, validation_data=(test_data, test_labels)\n",
        ")\n",
        "\n",
        "model.save_weights('model_weight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-TKhSCtGvR2",
        "outputId": "19b3c9ca-46bc-45df-c863-e5cb1364011c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "31/32 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.2389 - categorical_tp: 474.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:2699: UserWarning: Metric CatgoricalTP implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r32/32 [==============================] - 7s 38ms/step - loss: 0.0313 - accuracy: 0.2351 - categorical_tp: 474.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 1s 42ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.2629 - categorical_tp: 530.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.1597 - categorical_tp: 322.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.3214 - categorical_tp: 648.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.9167 - categorical_tp: 1848.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.7907 - categorical_tp: 1594.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 0.0313 - accuracy: 0.9762 - categorical_tp: 1968.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 1s 36ms/step - loss: 0.0313 - accuracy: 0.1746 - categorical_tp: 352.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.0313 - accuracy: 0.6409 - categorical_tp: 1292.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 1s 29ms/step - loss: 0.0313 - accuracy: 0.1706 - categorical_tp: 344.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.1845 - categorical_tp: 372.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.3839 - categorical_tp: 774.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.2619 - categorical_tp: 528.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.8264 - categorical_tp: 1666.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 1s 36ms/step - loss: 0.0313 - accuracy: 0.3690 - categorical_tp: 744.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 1s 47ms/step - loss: 0.0313 - accuracy: 0.9762 - categorical_tp: 1968.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 1s 46ms/step - loss: 0.0313 - accuracy: 0.7500 - categorical_tp: 1512.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 2s 51ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 1s 39ms/step - loss: 0.0313 - accuracy: 0.2331 - categorical_tp: 470.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.8829 - categorical_tp: 1780.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.7431 - categorical_tp: 1498.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.3254 - categorical_tp: 656.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.2629 - categorical_tp: 530.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.6726 - categorical_tp: 1356.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.9762 - categorical_tp: 1968.0000 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_categorical_tp: 852.0000\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.0313 - accuracy: 0.8046 - categorical_tp: 1622.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.2044 - categorical_tp: 412.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.8056 - categorical_tp: 1624.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 1s 28ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 2s 62ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 1s 42ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 1s 40ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 1s 37ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 1s 37ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0313 - accuracy: 0.0238 - categorical_tp: 48.0000 - val_loss: 0.0233 - val_accuracy: 0.0139 - val_categorical_tp: 12.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ia_jhWgFBrj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MsZ04osnHrBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}